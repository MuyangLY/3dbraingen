{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import nibabel as nib\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import dataloader\n",
    "from nilearn import plotting\n",
    "# from ADNI_dataset import *\n",
    "from BRATS_dataset_copy import *\n",
    "# from ATLAS_dataset import *\n",
    "from Model_alphaWGAN_copy import Discriminator, Generator, Code_Discriminator, Discriminator_content\n",
    "# from losses import losses_computer\n",
    "from copy import deepcopy\n",
    "from models import Discriminator_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=4\n",
    "gpu = True\n",
    "workers = 0\n",
    "\n",
    "LAMBDA= 10\n",
    "_eps = 1e-15\n",
    "Use_BRATS=True\n",
    "Use_ATLAS = False\n",
    "\n",
    "#setting latent variable sizes\n",
    "latent_dim = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainset = ADNIdataset(augmentation=True)\n",
    "# train_loader = torch.utils.data.DataLoader(trainset,batch_size=BATCH_SIZE,\n",
    "#                                           shuffle=True,num_workers=workers)\n",
    "if Use_BRATS:\n",
    "    #'flair' or 't2' or 't1ce'\n",
    "    trainset = BRATSdataset(imgtype='flair')\n",
    "    train_loader = torch.utils.data.DataLoader(trainset,batch_size = BATCH_SIZE, shuffle=True,\n",
    "                                               num_workers=workers)\n",
    "# if Use_ATLAS:\n",
    "#     trainset = ATLASdataset(augmentation=True)\n",
    "#     train_loader = torch.utils.data.DataLoader(trainset,batch_size=BATCH_SIZE,\n",
    "#                                           shuffle=True,num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_train_gen(data_loader):\n",
    "    while True:\n",
    "        for _,images in enumerate(data_loader):\n",
    "            yield images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_load = inf_train_gen(train_loader)\n",
    "batch= gen_load.__next__()  # real_images[0]:images  real_images[1]:maskes\n",
    "real_images=batch[0]  #数据集有问题\n",
    "real_masks=batch[1]\n",
    "# print(real_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Discriminator (2+4 blocks) with 1322180 parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Discriminator_test(\n",
       "  (body_ll): ModuleList(\n",
       "    (0): D_block(\n",
       "      (activ): LeakyReLU(negative_slope=0.2)\n",
       "      (conv1): Conv3d(5, 5, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (conv2): Conv3d(5, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (norm1): BatchNorm3d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (norm2): BatchNorm3d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (down): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
       "      (conv_sc): Conv3d(5, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (1): D_block(\n",
       "      (activ): LeakyReLU(negative_slope=0.2)\n",
       "      (conv1): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (conv2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (norm1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (norm2): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (down): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
       "      (conv_sc): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (body_content): ModuleList(\n",
       "    (0): D_block(\n",
       "      (activ): LeakyReLU(negative_slope=0.2)\n",
       "      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (conv2): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (norm2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (down): Identity()\n",
       "      (conv_sc): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (1): D_block(\n",
       "      (activ): LeakyReLU(negative_slope=0.2)\n",
       "      (conv1): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (conv2): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (down): Identity()\n",
       "      (conv_sc): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (2): D_block(\n",
       "      (activ): LeakyReLU(negative_slope=0.2)\n",
       "      (conv1): Conv3d(512, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (conv2): Conv3d(512, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (norm1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (norm2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (down): Identity()\n",
       "      (conv_sc): Identity()\n",
       "    )\n",
       "    (3): D_block(\n",
       "      (activ): LeakyReLU(negative_slope=0.2)\n",
       "      (conv1): Conv3d(512, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (conv2): Conv3d(512, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (norm1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (norm2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (down): Identity()\n",
       "      (conv_sc): Identity()\n",
       "    )\n",
       "  )\n",
       "  (body_layout): ModuleList(\n",
       "    (0): D_block(\n",
       "      (activ): LeakyReLU(negative_slope=0.2)\n",
       "      (conv1): Conv3d(128, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (conv2): Conv3d(1, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (norm1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (norm2): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (down): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
       "      (conv_sc): Conv3d(128, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (1): D_block(\n",
       "      (activ): LeakyReLU(negative_slope=0.2)\n",
       "      (conv1): Conv3d(1, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (conv2): Conv3d(1, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (norm1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (norm2): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (down): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
       "      (conv_sc): Conv3d(1, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (2): D_block(\n",
       "      (activ): LeakyReLU(negative_slope=0.2)\n",
       "      (conv1): Conv3d(1, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (conv2): Conv3d(1, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (norm1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (norm2): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (down): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
       "      (conv_sc): Conv3d(1, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    )\n",
       "    (3): D_block(\n",
       "      (activ): LeakyReLU(negative_slope=0.2)\n",
       "      (conv1): Conv3d(1, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (conv2): Conv3d(1, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (norm1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (norm2): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (down): AvgPool3d(kernel_size=2, stride=2, padding=0)\n",
       "      (conv_sc): Conv3d(1, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (rgb_to_features): ModuleList()\n",
       "  (final_ll): ModuleList(\n",
       "    (0): Conv3d(32, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (1): Conv3d(64, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  )\n",
       "  (final_content): ModuleList(\n",
       "    (0): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (1): Conv3d(512, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (2): Conv3d(512, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (3): Conv3d(512, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  )\n",
       "  (final_layout): ModuleList(\n",
       "    (0): Conv3d(1, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (1): Conv3d(1, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (2): Conv3d(1, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (3): Conv3d(1, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# writer = SummaryWriter('save') \n",
    "\n",
    "G = Generator(noise = latent_dim)\n",
    "CD = Code_Discriminator(code_size = latent_dim ,num_units = 4096)\n",
    "D = Discriminator(is_dis=True)\n",
    "D_L = Discriminator_test()\n",
    "E = Discriminator(out_class = latent_dim,is_dis=False)\n",
    "# G_L = GeneratorUNet()\n",
    "\n",
    "G.cuda()\n",
    "D.cuda()\n",
    "CD.cuda()\n",
    "E.cuda()\n",
    "# G_L.cuda()\n",
    "D_L.cuda()\n",
    "\n",
    "# dummy_input = Variable(torch.randn((4,latent_dim)),volatile=True).cuda()\n",
    "# mask = torch.zeros(4, 1, 64, 64, 64).cuda()\n",
    "# dummy_input = torch.rand(4, 1, 64, 64, 64).cuda()  # 网络中输入的数据维度\n",
    "# input_i = torch.rand(4, 1, 64, 64, 64).cuda()\n",
    "# dummy_input = torch.cat((input_i, mask), 1)\n",
    "\n",
    "# with SummaryWriter(comment='D_11') as w:\n",
    "#     w.add_graph(D, (dummy_input)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_optimizer = optim.Adam(G.parameters(), lr=0.0002)\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=0.0002)\n",
    "e_optimizer = optim.Adam(E.parameters(), lr = 0.0002)\n",
    "cd_optimizer = optim.Adam(CD.parameters(), lr = 0.0002)\n",
    "# g_l_optimizer = optim.Adam(G_L.parameters(), lr = 0.0002)\n",
    "d_l_optimizer = optim.Adam(D_L.parameters(), lr = 0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(model, x, x_gen, w=10):\n",
    "    \"\"\"WGAN-GP gradient penalty\"\"\"\n",
    "    assert x.size()==x_gen.size(), \"real and sampled sizes do not match\"\n",
    "    alpha_size = tuple((len(x), *(1,)*(x.dim()-1)))\n",
    "    alpha_t = torch.cuda.FloatTensor if x.is_cuda else torch.Tensor\n",
    "    alpha = alpha_t(*alpha_size).uniform_()\n",
    "    x_hat = x.data*alpha + x_gen.data*(1-alpha)\n",
    "    x_hat = Variable(x_hat, requires_grad=True)\n",
    "\n",
    "    def eps_norm(x):\n",
    "        x = x.view(len(x), -1)\n",
    "        return (x*x+_eps).sum(-1).sqrt()\n",
    "    def bi_penalty(x):\n",
    "        return (x-1)**2\n",
    "    \n",
    "    grad_xhat = torch.autograd.grad(model(x_hat)[\"layout\"].sum(), x_hat, create_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    penalty = w*bi_penalty(eps_norm(grad_xhat)).mean()\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wgan_loss(output, real, forD):\n",
    "    if real and forD:\n",
    "        ans = output.mean()\n",
    "    elif not real and forD:\n",
    "        ans = -output.mean()\n",
    "    elif real and not forD:\n",
    "        ans = -output.mean()\n",
    "    elif not real and not forD:\n",
    "        raise ValueError(\"gen loss should be for real\")\n",
    "    #print(real, forD, ans)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class losses_computer():\n",
    "    def __init__(self, opt, num_blocks):\n",
    "        \"\"\"\n",
    "        The class implementing the loss computations\n",
    "        \"\"\"\n",
    "        self.loss_function = wgan_loss()  # 选择损失函数                              \n",
    "        self.no_masks = False\n",
    "        # self.no_DR = opt.no_DR\n",
    "        self.lambdas = {\"content\": 0.5 / num_blocks,\n",
    "                        \"layout\": 0.5 / num_blocks,\n",
    "                        \"low-level\": 1.0 / num_blocks}\n",
    "\n",
    "    # def get_loss_function(self, loss_mode):\n",
    "        # if loss_mode == \"wgan\":\n",
    "        #     return wgan_loss\n",
    "        # elif loss_mode == \"hinge\":\n",
    "        #     return hinge_loss\n",
    "        # elif loss_mode == \"bce\":\n",
    "        #     return bce_loss\n",
    "        # else:\n",
    "        #     raise ValueError('Unexpected loss_mode {}'.format(mode))\n",
    "\n",
    "    def content_segm_loss(self, out_d, data, real, forD):  # out_d --> Dcon 的输出 [15, 4, 1, 1] data --> G的输出\n",
    "        \"\"\"\n",
    "        The multi-class cross-entropy loss used in the content masked attention\n",
    "        \"\"\"\n",
    "        mask = data\n",
    "        mask_ch = mask.shape[1]\n",
    "        if real:\n",
    "            ground_t = torch.arange(mask_ch).unsqueeze(1).unsqueeze(2).unsqueeze(3).unsqueeze(4)  #[ch, 1, 1, 1]\n",
    "            ground_t = ground_t.repeat(1, 1, out_d.shape[2], out_d.shape[3], out_d.shape[4])\n",
    "            ground_t = ground_t.repeat_interleave(mask.shape[0], dim=0)[:, 0, :, :, :]  # 沿着指定的维度重复张量的元素  [ch*mask.shape[0], 1, 1]\n",
    "        else:  # fake\n",
    "            ground_t = torch.ones_like(out_d)[:, 0, :, :, :] * mask_ch\n",
    "        weights = torch.cat((1 / (torch.sum(mask.detach(), dim=(0, 2, 3, 4))), torch.Tensor([1.0]).to(out_d.device)))\n",
    "        weights[weights == float('inf')] = 0\n",
    "        loss = F.cross_entropy(out_d, ground_t.long().to(out_d.device), weight=weights.to(out_d.device))\n",
    "        return loss\n",
    "\n",
    "    # def diversity_regularization(self, fake):\n",
    "    #     \"\"\"\n",
    "    #     The diversity regularization applied in the feature space of the generator\n",
    "    #     \"\"\"\n",
    "    #     loss = torch.nn.L1Loss()\n",
    "    #     ans = 0\n",
    "    #     for i in range(len(fake)):\n",
    "    #         for k in range(fake[i].shape[0]):\n",
    "    #             for m in range(k + 1, fake[i].shape[0]):\n",
    "    #                 ans += -loss(fake[i][k], fake[i][m])\n",
    "    #     return ans * 2 / (len(fake) * (len(fake) - 1))\n",
    "\n",
    "    def balance_losses(self, losses):\n",
    "        \"\"\"\n",
    "        Multiply each loss part with its lambda\n",
    "        \"\"\"\n",
    "        for item in losses:\n",
    "            if item in self.lambdas.keys():\n",
    "                losses[item] = losses[item] * self.lambdas[item]\n",
    "        return losses\n",
    "\n",
    "    def __call__(self, out_d, mask, real, forD):\n",
    "        losses = {}\n",
    "        # --- adversarial loss ---#\n",
    "        for item in out_d:\n",
    "            for i in range(len(out_d[item])):\n",
    "                if item == \"content\" and not self.no_masks:\n",
    "                    losses[item] = losses.get(item, 0) + self.content_segm_loss(out_d[item][i], mask, real, forD)  # .get() 返回键值并赋值0\n",
    "                else:\n",
    "                    losses[item] = losses.get(item, 0) + self.loss_function(out_d[item][i], real, forD)\n",
    "\n",
    "        # --- diversity regularization ---#\n",
    "        # if not forD and not self.no_DR:\n",
    "        #     losses[\"DR\"] = self.diversity_regularization(data[\"features\"])\n",
    "        losses = self.balance_losses(losses)\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def content_segm_loss(out_d, data, real):\n",
    "    \"\"\"\n",
    "    The multi-class cross-entropy loss used in the content masked attention\n",
    "    \"\"\"\n",
    "    mask = data\n",
    "    mask_ch = mask.shape[1]\n",
    "    if real:\n",
    "        ground_t = torch.arange(mask_ch).unsqueeze(1).unsqueeze(2).unsqueeze(3).unsqueeze(4)  # [1, 1, 1, 1, 1]\n",
    "        ground_t = ground_t.repeat(1, 1, out_d.shape[2], out_d.shape[3], out_d.shape[4])\n",
    "        ground_t = ground_t.repeat_interleave(mask.shape[0], dim=0)[:, 0, :, :, :]  # 沿着指定的维度重复张量的元素\n",
    "    else:  # fake\n",
    "        ground_t = torch.ones_like(out_d)[:, 0, :, :, :] * mask_ch\n",
    "    # weights = torch.cat((1 / (torch.sum(mask.detach(), dim=(0, 2, 3, 4))), torch.Tensor([1.0]).to(out_d.device)))\n",
    "    weights = 1 / (torch.sum(mask.detach(), dim=(1, 2, 3, 4)))\n",
    "    weights[weights == float('inf')] = 0\n",
    "    # weights = weights.max()\n",
    "    loss = F.cross_entropy(out_d, ground_t.long().to(out_d.device), weight=weights.to(out_d.device))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_y = Variable(torch.ones((BATCH_SIZE, 1)).cuda(non_blocking=True))\n",
    "# fake_y = Variable(torch.zeros((BATCH_SIZE, 1)).cuda(non_blocking=True))\n",
    "\n",
    "\n",
    "# criterion_con = losses_computer()\n",
    "criterion_bce = nn.BCELoss()\n",
    "criterion_l1 = nn.L1Loss()\n",
    "criterion_mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [5, 64, 64, 64] at entry 0 and [3, 64, 64, 64] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15412\\797110674.py\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mgen_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mreal_images\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mreal_masks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15412\\2143594522.py\u001b[0m in \u001b[0;36minf_train_gen\u001b[1;34m(data_loader)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minf_train_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimages\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Program Files\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    650\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 652\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Program Files\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Program Files\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\Program Files\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Program Files\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Program Files\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [5, 64, 64, 64] at entry 0 and [3, 64, 64, 64] at entry 1"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "torch.autograd.set_detect_anomaly = True\n",
    "g_iter = 1\n",
    "d_iter = 1\n",
    "cd_iter =1\n",
    "# mask = torch.zeros(4, 1, 64, 64, 64).cuda()\n",
    "TOTAL_ITER = 200000\n",
    "gen_load = inf_train_gen(train_loader)\n",
    "for iteration in range(TOTAL_ITER):\n",
    "\n",
    "    ###############################################\n",
    "    # Train Encoder - Generator \n",
    "    ###############################################\n",
    "    for p in D.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in CD.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in E.parameters():\n",
    "        p.requires_grad = True\n",
    "    for p in G.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    for p in D_L.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    ############方案一F#############\n",
    "    # for p in G_L.parameters():\n",
    "    #     p.requires_grad = True\n",
    "    ###############################\n",
    "\n",
    "    for iters in range(g_iter):\n",
    "        logits, losses = dict(), dict()\n",
    "        G.zero_grad()\n",
    "        E.zero_grad()\n",
    "        batch= gen_load.__next__()\n",
    "        real_images=batch[0]\n",
    "        real_masks=batch[1]\n",
    "        _batch_size = real_images.size(0)\n",
    "        real_images = Variable(real_images,volatile=True).cuda(non_blocking=True)\n",
    "        real_masks = Variable(real_masks,volatile=True).cuda(non_blocking=True)\n",
    "        z_rand = Variable(torch.randn((_batch_size,latent_dim)),volatile=True).cuda()  # 随机向量 Zr\n",
    "        z_hat = E(real_images).view(_batch_size,-1)  # 编码向量 Ze\n",
    "        x_hat, m_hat = G(z_hat)  # Xrec\n",
    "        x_rand, m_rand = G(z_rand)  # Xrand\n",
    "\n",
    "        #########################方案一F#################################\n",
    "        # y_hat = G_L(x_hat)  # Xrec-label\n",
    "        # y_rand = G_L(x_rand)  # Xrand-label\n",
    "\n",
    "        # xy_hat = torch.cat((x_hat, y_hat), 1)  # 拼接图像与标签\n",
    "        # xy_rand = torch.cat((x_rand, y_rand), 1)\n",
    "\n",
    "        # xm_hat = torch.cat((x_hat, mask), 1)  # 拼接图像与掩膜\n",
    "        # xm_rand = torch.cat((x_rand, mask), 1)\n",
    "        ################################################################\n",
    "\n",
    "        logits[\"G_h\"] = D_L(x_hat, m_hat, for_real=False, epoch=iteration)\n",
    "        logits[\"G_r\"] = D_L(x_rand, m_rand, for_real=False, epoch=iteration)\n",
    "\n",
    "        losses[\"G_h\"] = losses_computer(logits[\"G_h\"], m_hat, real=True, forD=False)\n",
    "        losses[\"G_r\"] = losses_computer(logits[\"G_r\"], m_rand, real=True, forD=False)\n",
    "\n",
    "        c_loss = -CD(z_hat).mean()\n",
    "\n",
    "\n",
    "        # d_real_loss = D(x_hat).mean()\n",
    "        # d_fake_loss = D(x_rand).mean()\n",
    "        d_real_loss = losses[\"G_h\"][\"low-level\"] + losses[\"G_h\"][\"layout\"]\n",
    "        d_fake_loss = losses[\"G_r\"][\"low-level\"] + losses[\"G_r\"][\"layout\"]\n",
    "\n",
    "        d_loss = d_fake_loss+d_real_loss  # -Eze[D(G(Ze))]-Ezr[D(G(Zr))]\n",
    "        l1_loss =10* criterion_l1(x_hat,real_images)  # \n",
    "        loss1 = l1_loss + c_loss + d_loss + losses[\"G_h\"][\"content\"] \\\n",
    "                + losses[\"G_r\"][\"content\"]\n",
    "\n",
    "\n",
    "        loss1.backward(retain_graph=True)\n",
    "        \n",
    "\n",
    "\n",
    "    ###############################################\n",
    "    # Train D\n",
    "    ###############################################\n",
    "    for p in D.parameters():  \n",
    "        p.requires_grad = True\n",
    "    for p in CD.parameters():  \n",
    "        p.requires_grad = False\n",
    "    for p in E.parameters():  \n",
    "        p.requires_grad = False\n",
    "    for p in G.parameters():  \n",
    "        p.requires_grad = False\n",
    "\n",
    "    for p in D_L.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    # for p in G_L.parameters():\n",
    "    #     p.requires_grad = False\n",
    "\n",
    "    for iters in range(d_iter):\n",
    "        d_optimizer.zero_grad()\n",
    "        d_l_optimizer.zero_grad()\n",
    "        batch= gen_load.__next__()\n",
    "        real_images=batch[0]\n",
    "        real_masks=batch[1]\n",
    "        _batch_size = real_images.size(0)\n",
    "        z_rand = Variable(torch.randn((_batch_size,latent_dim)),volatile=True).cuda()\n",
    "        real_images = Variable(real_images,volatile=True).cuda(non_blocking=True)\n",
    "        real_masks = Variable(real_masks,volatile=True).cuda(non_blocking=True)\n",
    "\n",
    "        z_hat = E(real_images).view(_batch_size,-1)\n",
    "        x_hat, m_hat = G(z_hat)\n",
    "        x_rand, m_rand = G(z_rand)\n",
    "\n",
    "        logits[\"D_h\"] = D_L(x_hat, m_hat, for_real=False, epoch=iteration)\n",
    "        logits[\"D_r\"] = D_L(x_rand, m_rand, for_real=False, epoch=iteration)\n",
    "        logits[\"D_i\"] = D_L(real_images, real_masks, for_real=True, epoch=iteration)\n",
    "\n",
    "        losses[\"D_h\"] = losses_computer(logits[\"D_h\"], m_hat, real=True, forD=True)\n",
    "        losses[\"D_r\"] = losses_computer(logits[\"D_r\"], m_rand, real=True, forD=True)\n",
    "        losses[\"D_i\"] = losses_computer(logits[\"D_i\"], m_rand, real=False, forD=True)\n",
    "\n",
    "\n",
    "        x_loss2 = 2*(losses[\"D_i\"][\"low-level\"] + losses[\"D_i\"][\"layout\"]) \\\n",
    "                    +losses[\"D_i\"][\"content\"]+sum(losses[\"D_h\"].values())  \\\n",
    "                    +sum(losses[\"D_r\"].values())\n",
    "        # x_loss2 = 2*D(real_images).mean()+D(x_hat).mean()+D(x_rand).mean()\n",
    "        gradient_penalty_r = calc_gradient_penalty(D_L,real_images.data, x_rand.data)\n",
    "        gradient_penalty_h = calc_gradient_penalty(D_L,real_images.data, x_hat.data)\n",
    "\n",
    "        loss2 = x_loss2+gradient_penalty_r+gradient_penalty_h\n",
    "        loss2.backward(retain_graph=True)\n",
    "\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # z_rand = Variable(torch.randn((_batch_size,latent_dim)),volatile=True).cuda()\n",
    "        # with torch.no_grad():\n",
    "        #     m_rand = G(z_rand)\n",
    "\n",
    "        d_l_optimizer.step()\n",
    "    ###############################################\n",
    "    # Train CD\n",
    "    ###############################################\n",
    "    \n",
    "    for p in D.parameters():  \n",
    "        p.requires_grad = False\n",
    "    for p in CD.parameters():  \n",
    "        p.requires_grad = True\n",
    "    for p in E.parameters():  \n",
    "        p.requires_grad = False\n",
    "    for p in G.parameters():  \n",
    "        p.requires_grad = False\n",
    "\n",
    "    for p in D_L.parameters():\n",
    "        p.requires_grad = False\n",
    "    # for p in G_L.parameters():\n",
    "    #     p.requires_grad = False\n",
    "\n",
    "    for iters in range(cd_iter):\n",
    "        cd_optimizer.zero_grad()\n",
    "        z_rand = Variable(torch.randn((_batch_size,latent_dim)),volatile=True).cuda()\n",
    "        gradient_penalty_cd = calc_gradient_penalty(CD,z_hat.data, z_rand.data)\n",
    "        loss3 = -CD(z_rand).mean() - c_loss + gradient_penalty_cd\n",
    "\n",
    "        loss3.backward(retain_graph=True)\n",
    "\n",
    "        e_optimizer.step()\n",
    "        g_optimizer.step()\n",
    "        g_optimizer.step()\n",
    "        cd_optimizer.step()\n",
    "\n",
    "    ###############################################\n",
    "    # Visualization\n",
    "    ###############################################\n",
    "\n",
    "    if iteration % 100 == 0:\n",
    "        print('[{}/{}]'.format(iteration,TOTAL_ITER),\n",
    "              'D: {:<8.3}'.format(loss2.item()), \n",
    "              'En_Ge: {:<8.3}'.format(loss1.item()),\n",
    "              'Code: {:<8.3}'.format(loss3.item()),\n",
    "              )\n",
    "        feat = np.squeeze((0.5*real_images[0]+0.5).data.cpu().numpy())\n",
    "        feat = nib.Nifti1Image(feat,affine = np.eye(4))\n",
    "        img_Xr = plotting.plot_img(feat,title=\"X_Real\")  \n",
    "        img_Xr.savefig(f'./checkpoint1/image/X_Real_{iteration+1}.png')    \n",
    "        # plotting.show()\n",
    "\n",
    "        feat = np.squeeze((0.5*x_hat[0]+0.5).data.cpu().numpy())\n",
    "        feat = nib.Nifti1Image(feat,affine = np.eye(4))\n",
    "        img_Xdec = plotting.plot_img(feat,title=\"X_DEC\")\n",
    "        img_Xdec.savefig(f'./checkpoint1/image/X_Dec_{iteration+1}.png') \n",
    "        # plotting.show()\n",
    "\n",
    "        feat = np.squeeze((0.5*x_rand[0]+0.5).data.cpu().numpy())\n",
    "        feat = nib.Nifti1Image(feat,affine = np.eye(4))\n",
    "        img_Xrand = plotting.plot_img(feat,title=\"X_rand\")\n",
    "        img_Xrand.savefig(f'./checkpoint1/image/X_Rand_{iteration+1}.png') \n",
    "        # plotting.show()\n",
    "\n",
    "        feat = np.squeeze((0.5*m_rand[0]+0.5).data.cpu().numpy())\n",
    "        feat = nib.Nifti1Image(feat,affine = np.eye(4))\n",
    "        img_Xrand = plotting.plot_img(feat,title=\"M_rand\")\n",
    "        img_Xrand.savefig(f'./checkpoint1/image/M_Rand_{iteration+1}.png') \n",
    "\n",
    "        feat = np.squeeze((0.5*m_hat[0]+0.5).data.cpu().numpy())\n",
    "        feat = nib.Nifti1Image(feat,affine = np.eye(4))\n",
    "        img_Xrand = plotting.plot_img(feat,title=\"M_DEC\")\n",
    "        img_Xrand.savefig(f'./checkpoint1/image/M_Dec_{iteration+1}.png') \n",
    "\n",
    "    ###############################################\n",
    "    # Model Save\n",
    "    ###############################################\n",
    "    if (iteration+1)%500 ==0: \n",
    "        torch.save(G.state_dict(),'./checkpoint1/weiget/G_iter'+str(iteration+1)+'.pth')\n",
    "        torch.save(D.state_dict(),'./checkpoint1/weiget/D_iter'+str(iteration+1)+'.pth')\n",
    "        torch.save(E.state_dict(),'./checkpoint1/weiget/E_iter'+str(iteration+1)+'.pth')\n",
    "        torch.save(CD.state_dict(),'./checkpoint1/weiget/CD_iter'+str(iteration+1)+'.pth')\n",
    "        torch.save(D_L.state_dict(),'./checkpoint1/weiget/DL_iter'+str(iteration+1)+'.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "abe97b4297a04eb9b59942013a51b5b6b6dd824dcfaba919a0d03fdddda38a68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
